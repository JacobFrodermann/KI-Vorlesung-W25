# NN04 - Overfitting und Regularisierung

> [!NOTE]
>
> <details>
>
> <summary><strong>ðŸŽ¦ Videos</strong></summary>
>
> - [NN4.1 - Nichtlineare Modelle](https://youtu.be/KJLT-h_ChRo)
> - [NN4.2 - Overfitting und
>   Regularisierung](https://youtu.be/BW91MYPUH_k)
>
> </details>
>
> <details>
>
> <summary><strong>ðŸ–‡ Unterlagen</strong></summary>
>
> - [NN04-Nichtlineare_Modelle_und_Overfitting.pdf](https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/lecture/nn/files/NN04-Nichtlineare_Modelle_und_Overfitting.pdf)
>
> </details>

## Kurze Ãœbersicht

### Nichtlineare Modelle

- EinfÃ¼hrung von neuen Merkmalen in Form von nichtlienaren Kombinationen
  der ursprÃ¼nglichen Merkmale
- ErhÃ¶hung der KomplexitÃ¤t des Modells ermÃ¶glicht das Erfassen von
  nichtlinearen Beziehungen
- **Bemerkung**: Die Hypothesenfunktion bleibt linear in den Gewichten,
  es wird weiterhin logistische Regression in einem **erweiterten**
  Merkmalraum durchgefÃ¼hrt.

### Ãœberanpassung und Regularisierung

- Die **Ãœberanpassung** (engl. Overfitting) ist eines der hÃ¤ufigsten und
  wichtigsten Probleme in ML und DL
- â€œWas im Bereich des maschinellen Lernens Professionelle von Amateuren
  unterscheidet, ist ihre FÃ¤higkeit mit Ãœberanpassung umzugehen.â€
  ([Abu-Mostafa, Magdon-Ismail, und Lin 2012, 119](#ref-AbuMostafa2012))
- Anzeichen von Ãœberanpassung sind geringe Trainingskosten und hohe
  **Testkosten** (Kosten auf nicht-gesehenen Daten).
- Regularisierung ist eine MaÃŸnahme gegen Ãœberanpassung. Man kann es
  sich als eine Reduktion in der KomplexitÃ¤t des Modells vorstellen.
- Der Regularisierungsparameter $`\lambda`$ ist ein Hyperparameter. Je
  grÃ¶ÃŸer der $`\lambda`$-Wert, desto grÃ¶ÃŸer der Regularisierungseffekt.
- Die **Kostentenfunktion** bei regulariserter logistischer Regression:

``` math
J = \frac{1}{m} \left\lbrack \sum_{i=1}^m \left( -y^{[i]}log(a^{[i]})-(1-y^{[i]})log(1-a^{[i]}) \right) + \frac{\lambda}{2} \sum_{j=1}^n (w^2_j)  \right\rbrack \tag{1}
```

- Die **Gewichtsaktualisierung** mit Regularisierungsterm:

``` math
w_j := w_j - \frac{\alpha}{m} \left\lbrack \sum_{i=1}^m \left( ( a^{[i]} - y^{[i]} )x_j^{[i]} \right) + \lambda w_j  \right\rbrack \tag{2}
```

## ðŸ“– Zum Nachlesen

- Abu-Mostafa, Magdon-Ismail, und Lin ([2012](#ref-AbuMostafa2012)):
  Kapitel 4

------------------------------------------------------------------------

> [!TIP]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k2: ErhÃ¶hung der Modell-KomplexitÃ¤t durch EinfÃ¼hrung von Merkmalen
>   hÃ¶herer Ordnung
> - k2: Unter- und Ãœberanpassung
> - k2: Regularisierung (Auswirkung auf Gewichte und Modell)
> - k3: Gradientenabstieg fÃ¼r regularisierte logistische Regression
>
> </details>
>
> <details>
>
> <summary><strong>ðŸ§© Quizzes</strong></summary>
>
> - [Selbsttest Overfitting
>   (ILIAS)](https://www.hsbi.de/elearning/goto.php?target=tst_1106595&client_id=FH-Bielefeld)
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>ðŸ‘€ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent"
> entry-spacing="0">
>
> <div id="ref-AbuMostafa2012" class="csl-entry">
>
> Abu-Mostafa, Y. S., M. Magdon-Ismail, und H. Lin. 2012. *Learning From
> Data*. AMLBook. <https://work.caltech.edu/telecourse>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> 6672880 (markdown: switch to leaner yaml header (#438), 2025-08-09)<br></sub></sup></p></blockquote>
