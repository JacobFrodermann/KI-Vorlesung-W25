# Alpha-Beta-Pruning

> [!NOTE]
>
> <details open>
>
> <summary><strong>ğŸ¯ TL;DR</strong></summary>
>
> Minimax entwickelt den gesamten Spielbaum. Wenn man dabei die bisher
> besten Werte fÃ¼r MAX und MIN als $`\alpha`$ und $`\beta`$ mitfÃ¼hrt,
> beobachtet man, dass ein $`\alpha`$-Wert nie kleiner wird und ein
> $`\beta`$-Wert nie grÃ¶ÃŸer wird. Dies kann man ausnutzen und das
> Entwickeln von Pfaden abbrechen, wenn in einem MIN-Knoten der
> $`\beta`$-Wert kleiner wird als der $`\alpha`$-Wert des
> MAX-VorgÃ¤ngers: (a) kann der $`\beta`$-Wert bei der weiteren
> Untersuchung der verbleibenden Nachfolger im MIN-Knoten nur noch
> kleiner werden, und (b) wÃ¼rde der MAX-VorgÃ¤nger diesen MIN-Knoten nie
> als Nachfolger in Betracht ziehen, da er bereits einen besseren Zug
> gesehen hat (da sein $`\alpha`$ grÃ¶ÃŸer ist als das $`\beta`$ im
> Nachfolger). Deshalb kann man hier sofort die Untersuchung der
> verbleibenden Nachfolger im MIN-Knoten abbrechen (â€œPruningâ€). Eine
> analoge Ãœberlegung gilt fÃ¼r einen MAX-Nachfolger unter einem
> MIN-Knoten.
>
> Dabei bleibt das Endergebnis erhalten. Man schneidet nur Suchpfade
> weg, die das Ergebnis von Minimax nicht verÃ¤ndern.
>
> Die mÃ¶gliche Effizienzsteigerung ist sehr stark abhÃ¤ngig von
> Sortierung der Nachfolger! Deshalb stellt man hÃ¤ufig Heuristiken zur
> â€œrichtigenâ€ Sortierung der Nachfolger auf (â€œKiller-Movesâ€).
>
> ZusÃ¤tzlich kann man wie bei Minimax auch die Suchtiefe begrenzen und
> eine Bewertungsfunktion (statt der Nutzenfunktion) einsetzen. Auch
> hier kann die Bewertungsfunktion wieder gewichtete Features nutzen
> und/oder Positionen mit in Datenbanken gespeicherten Positionen und
> Bewertungen abgleichen.
>
> </details>
>
> <details>
>
> <summary><strong>ğŸ¦ Videos</strong></summary>
>
> - [VL Alpha-Beta-Pruning](https://youtu.be/)
>
> </details>

## Verbesserung Minimax-Algorithmus

<img src="images/minimax.png" width="40%">

=\> **Minimax-Baum**: Verbesserungen mÃ¶glich?

## Alpha-beta-Pruning

Minimax-Algorithmus mit zusÃ¤tzlichen Informationen:

- $`\alpha`$: bisher bester Wert fÃ¼r MAX (hÃ¶chster Wert)
- $`\beta`$: bisher bester Wert fÃ¼r MIN (kleinster Wert)

=\> Beobachtungen:

1.  $`\alpha`$ fÃ¼r MAX-Knoten wird nie kleiner
2.  $`\beta`$ fÃ¼r MIN-Knoten wird nie grÃ¶ÃŸer

## Pruning-Regeln

1.  Schneide (unter) MIN-Knoten ab, deren $`\beta`$ $`\le`$ dem
    $`\alpha`$ des MAX-VorgÃ¤ngers ist.

<!-- -->

1.  Schneide (unter) MAX-Knoten ab, deren $`\alpha`$ $`\ge`$ dem
    $`\beta`$ des MIN-VorgÃ¤ngers ist.

<div align="center">

Abbruch, wenn kein Platz mehr zwischen Alpha und Beta

</div>

## Alpha-beta-Pruning â€“ Der Algorithmus (Skizze)

``` python
def Max-Value(state, alpha, beta):
    if Terminal-Test(state): return Utility(state)

    v = -INF
    for (a, s) in Successors(state):
        v = MAX(v, Min-Value(s, alpha, beta))
        if (v >= beta): return v
        alpha = MAX(alpha, v)
    return v
```

``` python
def Min-Value(state, alpha, beta):
    if Terminal-Test(state): return Utility(state)

    v = +INF
    for (a, s) in Successors(state):
        v = MIN(v, Max-Value(s, alpha, beta))
        if (v <= alpha): return v
        beta = MIN(beta, v)
    return v
```

Initialer Aufruf von `Max-Value()` mit $`\alpha = -\infty`$ und
$`\beta = +\infty`$

**Achtung**: Es kursieren Varianten von diesem Algorithmus, bei denen
auf die Hilfsvariable `v` verzichtet wird und stattdessen `alpha` bzw.
`beta` direkt modifiziert werden und als RÃ¼ckgabewert dienen. Das *kann*
zu anderen oder falschen Ergebnissen fÃ¼hren! Sie kÃ¶nnen das in der
Aufgabe auf Blatt 03 gut beobachten.

## Alpha-beta-Pruning â€“ Eigenschaften

1.  Pruning beeinflusst nicht das Endergebnis!

2.  Sortierung der Nachfolger spielt groÃŸe Rolle

3.  Perfekte Sortierung: $`O(b^{d/2})`$ =\> Verdopplung der Suchtiefe
    mÃ¶glich

FÃ¼r Schach immer noch zu aufwÃ¤ndig â€¦

## Verbesserungen fÃ¼r Alpha-beta-Pruning

- â€œKiller-Moveâ€: Maximale Effizienz nur wenn **optimaler Zug immer
  zuerst** untersucht =\> Zu untersuchende ZÃ¼ge
  **sortieren/priorisieren**, zb. Schach:
  1.  Figuren schlagen
  2.  Drohen
  3.  VorwÃ¤rts ziehen
  4.  RÃ¼ckwÃ¤rts ziehen

<!-- -->

- VerÃ¤ndern der Suchtiefe nach Spielsituation

<!-- -->

- Bewertungsfunktion `Eval`:
  - Datenbanken mit Spielsituationen und Expertenbewertung:
    - ErÃ¶ffnungsspiele (besonders viele Verzweigungen)
    - Endspiele
  - Lernen der optimalen Gewichte fÃ¼r `Eval`-Funktion
  - BerÃ¼cksichtigung von Symmetrien

## Beispiel DeepBlue (IBM, 1997)

- Alpha-beta-Pruning mit TiefenbeschrÃ¤nkung: ca. 14 HalbzÃ¼ge
- Dynamische TiefenbeschrÃ¤nkung (stellungsabhÃ¤ngig, max. ca. 40 ZÃ¼ge)
- Heuristische Stellungsbewertung `Eval`:
  - mehr als 8.000 Features
  - ca. 4.000 ErÃ¶ffnungsstellungen
  - ca. 700.000 Spielsituationen (von Experten bewertet)
  - Endspiel-Datenbank: alle Spiele mit 5 Steinen, viele mit 6 Steinen

Quelle: ([Russell und Norvig 2014](#ref-Russell2014), p.Â 185)

## Beispiel AlphaGo (Google, 2016)

- BeschrÃ¤nkung der Suchtiefe: Bewertung der Stellung durch *â€œValue
  Networkâ€*
- BeschrÃ¤nkung der Verzweigungsbreite: Bestimmung von Zugkandidaten
  durch *â€œPolicy Networkâ€*

<!-- -->

- Training dieser *â€œDeep Neural Networksâ€*:
  - Ãœberwachtes Lernen: â€œAnalyseâ€ von Spiel-Datenbanken
  - Reinforcement-Lernen: Self-Play, Bewertung am Ende
    - ZÃ¼ge mit Monte-Carlo-Baumsuche ausgewÃ¤hlt

Quelle: ([Silver u.Â a. 2016](#ref-Silver2016)), siehe auch
[deepmind.com/research/alphago/](https://deepmind.com/research/case-studies/alphago-the-story-so-far)

## Wrap-Up

- Alpha-beta-Pruning:
  - MitfÃ¼hren der bisher besten Werte fÃ¼r MAX und MIN: $`\alpha`$ und
    $`\beta`$
  - Abschneiden von Pfaden, die Verschlechterung bewirken wÃ¼rden
  - Endergebnis bleibt erhalten
  - Effizienzsteigerung abhÃ¤ngig von Sortierung der Nachfolger

<!-- -->

- Viele Verbesserungen denkbar:
  - Zu untersuchende ZÃ¼ge â€œrichtigâ€ sortieren (Heuristik)
  - Suchtiefe begrenzen und Bewertungsfunktion (statt Nutzenfunktion)
  - Positionen mit Datenbank abgleichen

## ğŸ“– Zum Nachlesen

- Russell und Norvig ([2020](#ref-Russell2020)): Alpha-beta-Pruning:
  Abschnitt 6.2.3, Erweiterungen: Abschnitt 6.3
- Ertel ([2017](#ref-Ertel2017))

------------------------------------------------------------------------

> [!TIP]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k2: OptimierungsmÃ¶glichkeit: Sortierung der Nachfolger =\> Heuristik
> - k2: OptimierungsmÃ¶glichkeit: Suchtiefe beschrÃ¤nken =\> Ãœbergang zu
>   Bewertungsfunktion
> - k2: OptimierungsmÃ¶glichkeit: Bewertung Ã¼ber Spieldatenbanken
> - k3: alpha-beta-Pruning
>
> </details>
>
> <details>
>
> <summary><strong>ğŸ§© Quizzes</strong></summary>
>
> - [Selbsttest Alpha-Beta-Pruning
>   (ILIAS)](https://www.hsbi.de/elearning/goto.php?target=tst_1106584&client_id=FH-Bielefeld)
>
> </details>
>
> <details>
>
> <summary><strong>ğŸ… Challenges</strong></summary>
>
> **Optimale Spiele und MiniMax**
>
> Auf einem Tisch liegen nebeneinander 5 StreichhÃ¶lzer. Es gibt zwei
> Spieler - WeiÃŸ und Schwarz - die abwechselnd ein oder zwei
> StreichhÃ¶lzer wegnehmen dÃ¼rfen (es muss mind. ein Streichholz genommen
> werden). Wer das letzte Streichholz nehmen muss, hat verloren. Zu
> Beginn ist WeiÃŸ am Zug.
>
> 1.  Spielbaum
>
> Zeichnen Sie den **kompletten** Spielbaum auf. Geben Sie an den Kanten
> jeweils die Zahl der genommenen und der verbleibenden HÃ¶lzer an.
>
> *Beispiel*: Wenn in einem Zug ein Holz genommen wird und 3 HÃ¶lzer
> verbleiben, steht an der entsprechenden Kante â€œ1/3â€. Geben Sie jeweils
> an, welcher Spieler am Zug ist.
>
> 1.  Minimax
>
> Geben Sie die Bewertung aller SpielzustÃ¤nde mit Hilfe des
> Minimax-Algorithmus an. Bewerten Sie die EndzustÃ¤nde mit +1, wenn
> Spieler WeiÃŸ gewonnen hat, mit -1, falls Schwarz gewonnen hat.
>
> 1.  Alpha-Beta-Pruning
>
> Wenden Sie Alpha-Beta-Pruning auf den Spielbaum an. Werden damit mehr
> oder weniger oder gleich viele SpielzÃ¼ge wie mit Minimax entwickelt?
> BegrÃ¼nden Sie Ihre Antwort.
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>ğŸ‘€ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent"
> entry-spacing="0">
>
> <div id="ref-Ertel2017" class="csl-entry">
>
> Ertel, W. 2017. *Introduction to Artificial Intelligence*. 2nd
> edition. Springer. <https://doi.org/10.1007/978-3-319-58487-4>.
>
> </div>
>
> <div id="ref-Russell2014" class="csl-entry">
>
> Russell, S., und P. Norvig. 2014. *Artificial Intelligence: A Modern
> Approach*. 3rd revised edition. Pearson.
> <http://aima.cs.berkeley.edu>.
>
> </div>
>
> <div id="ref-Russell2020" class="csl-entry">
>
> â€”â€”â€”. 2020. *Artificial Intelligence: A Modern Approach*. 4th Edition.
> Pearson. <http://aima.cs.berkeley.edu>.
>
> </div>
>
> <div id="ref-Silver2016" class="csl-entry">
>
> Silver, D., A. Huang, C. Maddison, A. Guez, L. Sifre, G. Van Den
> Driessche, I. Schrittwieser J. Schrittwieser, V. Panneershelvam, M.
> Lanctot, u.Â a. 2016. â€Mastering the Game of Go with Deep Neural
> Networks and Tree Searchâ€œ. *Nature* 529 (7587): 484.
> <https://doi.org/10.1038/nature16961>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

**Exceptions:**

- ([Russell und Norvig 2014](#ref-Russell2014), p.Â 185)
- ([Silver u.Â a. 2016](#ref-Silver2016)), siehe auch
  [deepmind.com/research/alphago/](https://deepmind.com/research/case-studies/alphago-the-story-so-far)

<blockquote><p><sup><sub><strong>Last modified:</strong> 6672880 (markdown: switch to leaner yaml header (#438), 2025-08-09)<br></sub></sup></p></blockquote>
